{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook for NER exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all necessary packages in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raunakanand/Documents/Work_R/interview_problems/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/raunakanand/Documents/Work_R/interview_problems/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/raunakanand/Documents/Work_R/interview_problems/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import evaluate\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForTokenClassification,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          Trainer, \n",
    "                          TrainingArguments)\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Preprocessing & EDA - all of them executed in glob code cell for all datasets\n",
    "\n",
    "- quick data overview *(see function 'quick_eda')*\n",
    "\n",
    "- Removing duplicates\n",
    "\n",
    "- Removing rows where label's - start index and end index spill over text length or start index is greater than end index; *see function ('idx_to_remove')*\n",
    "\n",
    "- Processing 'tags' column of data which is string into tuples of entities index and text; *see function ('create_entities)*\n",
    "\n",
    "- preprocess functions which cleans data and creates some new columns *see function ('preprocess)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_remove(tags_list, text_list):\n",
    "    idx_to_remove = []\n",
    "\n",
    "    for i, x in enumerate(tags_list):\n",
    "        mx_idx = -1\n",
    "        for y in x:\n",
    "            start_idx = int(y.split(':')[0])\n",
    "            end_idx = int(y.split(':')[1])\n",
    "            mx_idx = max(mx_idx, start_idx, end_idx)\n",
    "\n",
    "            if(start_idx>end_idx):\n",
    "                # print(y.split(':'))\n",
    "                # print(i,x,\"====\", start_idx, end_idx)\n",
    "                idx_to_remove.append(i)\n",
    "            \n",
    "            if(mx_idx > len(text_list[i])+1):\n",
    "                # print(i, x, text_list[i])\n",
    "                idx_to_remove.append(i)\n",
    "\n",
    "    return idx_to_remove\n",
    "\n",
    "def quick_eda(df):\n",
    "    print(df.info(), \"\\n\")\n",
    "    print(\"No. of duplicates in dataframe - {}\".format(df.loc[df.duplicated()].shape[0]))\n",
    "    \n",
    "    tags = df['tags'].tolist()\n",
    "    text = df['text'].tolist()\n",
    "    tags = [[y for y in x.split(',') if len(y)!=0] for x in tags]\n",
    "    set_labels = set([y.split(':')[-1] for x in tags for y in x])\n",
    "    print(\"Unique labels in tags columns: {}\".format(set_labels),\"\\n\\n\")\n",
    "\n",
    "    return \n",
    "\n",
    "def create_entites(tags, text):\n",
    "    entities = []\n",
    "    for x in tags:\n",
    "        temp = []\n",
    "        for y in x:\n",
    "            temp_ls = y.split(':')\n",
    "            start_idx = int(temp_ls[0])\n",
    "            end_idx = int(temp_ls[1])\n",
    "            label = temp_ls[-1]\n",
    "            tup = (start_idx, end_idx, label)\n",
    "            # print(tup)\n",
    "            temp.append(tup)\n",
    "        entities.append(temp)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def preprocess(df):\n",
    "    print('original_shape: {}'.format(df.shape))\n",
    "    df = df.drop_duplicates()\n",
    "    print('shape after duplicates drop: {}'.format(df.shape))\n",
    "    tags = df['tags'].tolist()\n",
    "    text = df['text'].tolist()\n",
    "    tags = [[y for y in x.split(',') if len(y)!=0] for x in tags]\n",
    "    # print(tags[:5])\n",
    "    entities = create_entites(tags=tags, text=text)\n",
    "    df['entities'] = entities\n",
    "    remove_idx_ls = idxs_to_remove(tags, text)\n",
    "    print(\"{} indices to remove\".format(len(remove_idx_ls)))\n",
    "    print('Removing_index: {}'.format(remove_idx_ls))\n",
    "    df = df.drop(remove_idx_ls, axis=0).reset_index()\n",
    "\n",
    "    print(\"Final columns: {}\".format(df.columns))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running all the above EDA and processing function in the bleow glob code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_source/G1.xlsx \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7356 entries, 0 to 7355\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      7356 non-null   object\n",
      " 1   tags    7356 non-null   object\n",
      " 2   text    7356 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 172.5+ KB\n",
      "None \n",
      "\n",
      "No. of duplicates in dataframe - 167\n",
      "Unique labels in tags columns: {'treatment', 'allergy_name', 'cancer', 'chronic_disease'} \n",
      "\n",
      "\n",
      "original_shape: (7356, 3)\n",
      "shape after duplicates drop: (7189, 3)\n",
      "2 indices to remove\n",
      "Removing_index: [1522, 4236]\n",
      "Final columns: Index(['index', 'ID', 'tags', 'text', 'entities'], dtype='object')\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "../data_source/G3.xlsx \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6277 entries, 0 to 6276\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      6277 non-null   object\n",
      " 1   tags    6277 non-null   object\n",
      " 2   text    6277 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 147.2+ KB\n",
      "None \n",
      "\n",
      "No. of duplicates in dataframe - 0\n",
      "Unique labels in tags columns: {'treatment', 'allergy_name', 'cancer', 'chronic_disease'} \n",
      "\n",
      "\n",
      "original_shape: (6277, 3)\n",
      "shape after duplicates drop: (6277, 3)\n",
      "0 indices to remove\n",
      "Removing_index: []\n",
      "Final columns: Index(['index', 'ID', 'tags', 'text', 'entities'], dtype='object')\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "../data_source/G2.xlsx \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6454 entries, 0 to 6454\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      6454 non-null   object\n",
      " 1   tags    6454 non-null   object\n",
      " 2   text    6454 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 201.7+ KB\n",
      "None \n",
      "\n",
      "No. of duplicates in dataframe - 0\n",
      "Unique labels in tags columns: {'treatment', 'allergy_name', 'cancer', 'chronic_disease'} \n",
      "\n",
      "\n",
      "original_shape: (6454, 3)\n",
      "shape after duplicates drop: (6454, 3)\n",
      "2 indices to remove\n",
      "Removing_index: [3336, 3472]\n",
      "Final columns: Index(['index', 'ID', 'tags', 'text', 'entities'], dtype='object')\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Running all the cleaning and processing on all datasets with glob\n",
    "dfs = []\n",
    "for f in glob('../data_source/*.xlsx'):\n",
    "    print(f,\"\\n\")\n",
    "    df = pd.read_excel(f)\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "    df = df.dropna(axis=0)\n",
    "    quick_eda(df)\n",
    "    df = preprocess(df)\n",
    "\n",
    "    dfs.append(df)\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Next steps of Preprocessing - *check docstings of functions for details*\n",
    "- Changing our entities label and text into IOB format \n",
    "\n",
    "- IOB format is converting text into list/array of token and having a same length list/array of labels\n",
    "\n",
    "- IOB - where I denotes inside a label, B denotes beginning of a label - see below cell (*ner_label_iob*)\n",
    "\n",
    "- This transformation is used for training and evaluation - given we have selected transformers based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = ['chronic_disease', 'cancer', 'treatment', 'allergy_name']\n",
    "#ner lablels converted in iob format for data transformation into tokens and IOB entity list\n",
    "ner_label_iob = ['O', 'B-CHR', 'I-CHR', 'B-CAN', 'I-CAN', 'B-TRE', 'I-TRE', 'B-ALL', 'I-ALL']\n",
    "\n",
    "id2label = {k: v for k, v in enumerate(ner_label_iob)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_special_characters(s):\n",
    "    \"\"\"function to remove special characters which shall be used in get_token_iob_label_list,\n",
    "        to remove special characters from initial tokens, \n",
    "        so that bracketed/abbreviated disease names etc can be include with the disease label\"\"\"\n",
    "    \n",
    "    pattern = r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$'\n",
    "    result = re.sub(pattern, '', s)\n",
    "    return result\n",
    "\n",
    "def get_token_iob_label_list(text_list, entities_list):\n",
    "    \"\"\"function to convert [text and entites] into [token_list and IOB entities] list respectively.\n",
    "        text is converted to [spacy tokens list] \n",
    "        and the ent_text for the entites are put in a dictionary with ner_labels as keys.\n",
    "        Every elements from [spacy token list] is checked if they are in the text of ner_label in ent_text dictionary\n",
    "          and then using `beginning and inside flag` IOB entites are appended in [label_list]\"\"\"\n",
    "    \n",
    "    tokens_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for text, entities in zip(text_list, entities_list):\n",
    "        ents_text = {ele : \"\" for ele in ner_labels}\n",
    "\n",
    "        for y in entities:\n",
    "            # print(text[y[0]])\n",
    "            # print(text[y[0]-1:y[1]])\n",
    "            ents_text[y[-1]] += text[y[0]-1:y[1]]  ## start index always has one positive offset in data, hence y[0]-1\n",
    "\n",
    "        ents_text[y[-1]] = [remove_special_characters(item) for item in ents_text[y[-1]].split()]\n",
    "        # print(ents_text)\n",
    "        doc = nlp(text)\n",
    "        token_list = [token.text for token in doc]\n",
    "        label_list = ['O']*len(token_list)\n",
    "\n",
    "        cd_flag, c_flag, t_flag, a_flag = 0, 0, 0, 0\n",
    "        for i, token in enumerate(token_list):\n",
    "            if(token in ents_text['chronic_disease']):\n",
    "                if(cd_flag==0):\n",
    "                    label_list[i] = 'B-CHR'\n",
    "                    cd_flag = 1\n",
    "                    c_flag, t_flag, a_flag = 0, 0, 0\n",
    "                else:\n",
    "                    label_list[i] = 'I-CHR'\n",
    "            \n",
    "            elif(token in ents_text['cancer']):\n",
    "                if(c_flag==0):\n",
    "                    label_list[i] = 'B-CAN'\n",
    "                    c_flag = 1\n",
    "                    cd_flag, t_flag, a_flag = 0, 0, 0\n",
    "                else:\n",
    "                    label_list[i] = 'I-CAN'\n",
    "                \n",
    "            elif(token in ents_text['treatment']):\n",
    "                if(t_flag==0):\n",
    "                    label_list[i] = 'B-TRE'\n",
    "                    t_flag = 1\n",
    "                    cd_flag, c_flag, a_flag = 0, 0, 0\n",
    "                else:\n",
    "                    label_list[i] = 'I-TRE'\n",
    "                \n",
    "            elif(token in ents_text['allergy_name']):\n",
    "                if(a_flag==0):\n",
    "                    label_list[i] = 'B-ALL'\n",
    "                    a_flag = 1\n",
    "                    cd_flag, c_flag, t_flag= 0, 0, 0\n",
    "                else:\n",
    "                    label_list[i] = 'I-ALL'\n",
    "            else:\n",
    "                cd_flag, c_flag, t_flag, a_flag = 0, 0, 0, 0\n",
    "                \n",
    "        assert(len(token_list)==len(label_list)), \"len of token_list and label_list mismatch at some iteration .\"\n",
    "        tokens_list.append(token_list)\n",
    "        labels_list.append([label2id[label] for label in label_list])\n",
    "\n",
    "    return (tokens_list, labels_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training\n",
    "\n",
    "- creating function to align labels with tokens because transformers tokenizer would spill our tokens to subtokens and then we need to have labels for those subtokens to be available in our tokenized data labels that will be fed for supervised training.\n",
    "\n",
    "- **['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']** with transformers tokeinzer would be converted to: <br> **['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']**\n",
    "\n",
    "- but entities we will provide will be of less length , to align those entity labels in tokenized datasets for same size as input_ids(tokens_list), we will use function *align_labels_with_tokens*\n",
    "\n",
    "- function *align_labels_with_tokens* - makes the entity labels for subwords created by tokenizer and gives them correct labels and length.\n",
    "\n",
    "- fucntion *tokenize and align* - just tokenizes all the datasetdict that we shall make for training loop and makes sures that ***inputs_ids and labels in input data*** are of same size and labels for extra tokens generated by tokenizer are correctly in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(word_ids, labels):\n",
    "    new_labels = []\n",
    "    last_word = None\n",
    "    for word_id in word_ids:\n",
    "        if(word_id is None):\n",
    "            new_labels.append(-100)\n",
    "            last_word = None\n",
    "        else:\n",
    "            if(word_id!=last_word):\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "                last_word=word_id\n",
    "            else:\n",
    "                label = labels[word_id]\n",
    "                if(label % 2 == 1):\n",
    "                    label += 1\n",
    "                new_labels.append(label)\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align(examples):\n",
    "    \"\"\"tokenizer function that shall be used to map datasetdicts into tokenized data with tensors, truncated and padded;\n",
    "      and well aligned input_labels and labels, before training\"\"\"\n",
    "    \n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, label in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        labels = align_labels_with_tokens(word_ids=word_ids, labels=label)\n",
    "        new_labels.append(labels)\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute metrics for training pipeline to evaluate metrics on validations set while training\"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens like [CLS] and [SEP] or any unknown tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7187, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:32, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7187, 5) 7187 7187\n",
      "(6277, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:00, 29.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6277, 5) 6277 6277\n",
      "(6452, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:27, 29.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6452, 5) 6452 6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## looping over all dataset to convert existing dataset into IOB format for entites and text into list of tokens for training with transformers model\n",
    "\n",
    "for i, x in tqdm(enumerate(dfs)):\n",
    "    print(x.shape)\n",
    "    tokens, labels = get_token_iob_label_list(x['text'].tolist(), x['entities'].tolist())\n",
    "    # dfs[i] = pd.DataFrame(data = {'tokens': tokens,\n",
    "    #                               'labels': labels})\n",
    "    print(x.shape, len(tokens), len(labels))\n",
    "    x['tokens'] = tokens\n",
    "    x['ner_tags'] = labels\n",
    "    dfs[i] = x[['tokens', 'ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creating datasetdicts for continous training with 100 samples moving to every next train dataset\n",
    "and train_test_val_split for every dataset and creating another `test_dataset_including_prev_tests` dataset including test data of previous datasets\n",
    "for evaluation\n",
    "\"\"\"\n",
    "\n",
    "ls_datadicts = []\n",
    "for i, x in enumerate(dfs):\n",
    "    temp_datadict = DatasetDict({'train' : Dataset.from_pandas(x)})\n",
    "    # print(temp_datadict)\n",
    "    if(i-1>=0):\n",
    "        sample_prev_100 = ls_datadicts[i-1]['train'].select(np.random.randint(0, (ls_datadicts[i-1]['train'].num_rows), size=100))\n",
    "        temp_datadict['train'] = concatenate_datasets([temp_datadict['train'], sample_prev_100])\n",
    "\n",
    "    temp_datadict = temp_datadict['train'].train_test_split(0.2, seed=42, shuffle=True)\n",
    "    val_data = temp_datadict['train'].train_test_split(0.1, shuffle=True)\n",
    "    temp_datadict['validation'] = val_data['test']\n",
    "    temp_datadict['train']= val_data['train']\n",
    "    temp_datadict['test_prevs'] = temp_datadict['test']\n",
    "    \n",
    "    if(i-1>=0):\n",
    "        temp_datadict['test_prevs'] = concatenate_datasets([temp_datadict['test_prevs'], ls_datadicts[i-1]['test_prevs']])\n",
    "\n",
    "    ls_datadicts.append(temp_datadict)\n",
    "    # print(ls_datadicts)\n",
    "\n",
    "final_dataset = Dataset.from_pandas(pd.concat(dfs)).train_test_split(test_size=0.2, seed=42, shuffle=True)\n",
    "final_dataset = final_dataset.remove_columns(column_names=['__index_level_0__'])\n",
    "val_data = final_dataset['train'].train_test_split(0.1, shuffle=True)\n",
    "final_dataset['validation'] = val_data['test']\n",
    "final_dataset['train']= val_data['train']\n",
    "final_dataset['test_prevs'] = ls_datadicts[-1]['test_prevs']\n",
    "ls_datadicts.append(final_dataset)\n",
    "\n",
    "ls_datadicts = ls_datadicts[-1:]   #only one item for memeory fot (G1+G2+G3) training, comment for full pipeline run with all models training \n",
    "print(len(ls_datadicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['tokens', 'ner_tags'],\n",
       "         num_rows: 14338\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['tokens', 'ner_tags'],\n",
       "         num_rows: 3984\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['tokens', 'ner_tags'],\n",
       "         num_rows: 1594\n",
       "     })\n",
       "     test_prevs: Dataset({\n",
       "         features: ['tokens', 'ner_tags'],\n",
       "         num_rows: 4025\n",
       "     })\n",
       " })]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_datadicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows 100 addition\n",
    "# print(dfs[1].shape[0])\n",
    "# print(ls_datadicts[1]['train'].num_rows + ls_datadicts[1]['test'].num_rows +ls_datadicts[1]['validation'].num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialising model check points in and out of the training loop, which will be consumed in every iteration of continous learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt_in = ['bert-base-uncased', 'raunak6898/bert-finetuned-ner-t1',\n",
    "                  'raunak6898/bert-finetuned-ner-t2','bert-base-cased']\n",
    "model_ckpt_eval = ['raunak6898/bert-finetuned-ner-t1',\n",
    "                  'raunak6898/bert-finetuned-ner-t2','raunak6898/bert-finetuned-ner-t3', 'raunak6898/bert-finetuned-ner-all_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-uncased',\n",
       " 'raunak6898/bert-finetuned-ner-t1',\n",
       " 'raunak6898/bert-finetuned-ner-t2',\n",
       " 'bert-base-cased']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "This was run in two breaks beacuse of MPS memory issues with the final (G1+G2+G3) training, the current \n",
    "code is left for last iter (iter = 3 in loop) with a break statement for the final training.\n",
    "\n",
    "- The training loop runs over the [model_ckpt_in] list and has corresponding [model_ckpt_eval] values that are model names pushed to huggingface-hub after every subsequent training\n",
    "\n",
    "- metric used is 'seqeval' sequence labeling evaluator, its standard norm for all sort of chunking tasks like ner evaluation. \n",
    "\n",
    "- result are stored in a list of dictionaries one for same test_set scores and another with current + prev test_sets scores\n",
    "\n",
    "- then that list of scores is converted into table of scores as desired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------Task 4 with base model -> bert-base-cased-----------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dcf67b669f49499a8d1526e7a64ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1026fdc46b64d58abd175463bc7bfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b0f9bcf4234915800765ccbf54686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976a740c72a64b2b819a7c0472789f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceaa550c830494398c52dade3dd00ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/4 [02:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5983, 'learning_rate': 1.8140918386317162e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [04:50<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4361, 'learning_rate': 1.6281836772634322e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [06:56<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3768, 'learning_rate': 1.4422755158951478e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee068a79cd54532a1a0c36d5489151c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [08:34<?, ?it/s]Checkpoint destination directory raunak6898/bert-finetuned-ner-all_data/checkpoint-1793 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3464497923851013, 'eval_precision': 0.5816847479259732, 'eval_recall': 0.5537667071688943, 'eval_f1': 0.5673825085589792, 'eval_accuracy': 0.8780963780183181, 'eval_runtime': 24.8476, 'eval_samples_per_second': 64.151, 'eval_steps_per_second': 8.049, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [09:25<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3382, 'learning_rate': 1.2563673545268638e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [11:22<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2999, 'learning_rate': 1.0704591931585797e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [13:20<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2919, 'learning_rate': 8.845510317902957e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [15:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2674, 'learning_rate': 6.986428704220116e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbe11f4e2bc4acba7e6ac9315e60cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [15:50<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3223128616809845, 'eval_precision': 0.5896296296296296, 'eval_recall': 0.6044957472660997, 'eval_f1': 0.5969701514924254, 'eval_accuracy': 0.8876977518734388, 'eval_runtime': 10.2839, 'eval_samples_per_second': 155.0, 'eval_steps_per_second': 19.448, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [17:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2335, 'learning_rate': 5.127347090537274e-06, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [19:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2139, 'learning_rate': 3.268265476854434e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [21:26<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2196, 'learning_rate': 1.4091838631715934e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a954dc2c7a04380b1ef96eceb721cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [23:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3217698037624359, 'eval_precision': 0.5944218477629285, 'eval_recall': 0.6215066828675577, 'eval_f1': 0.6076626076626077, 'eval_accuracy': 0.894150707743547, 'eval_runtime': 10.1386, 'eval_samples_per_second': 157.22, 'eval_steps_per_second': 19.727, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [23:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1383.198, 'train_samples_per_second': 31.097, 'train_steps_per_second': 3.889, 'train_loss': 0.3194687732158263, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177fdb0a39c749828ae800caab82d2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------Result on test_set of this dataset ------------------------------------------------------------- \n",
      "\n",
      " {'ALL': {'precision': 0.611764705882353, 'recall': 0.5621621621621622, 'f1': 0.5859154929577466, 'number': 185}, 'CAN': {'precision': 0.5642915642915642, 'recall': 0.5666118421052632, 'f1': 0.5654493229380386, 'number': 1216}, 'CHR': {'precision': 0.6043336301573167, 'recall': 0.6260762607626076, 'f1': 0.6150128379398881, 'number': 3252}, 'TRE': {'precision': 0.5913068021450748, 'recall': 0.6348484848484849, 'f1': 0.6123045447902967, 'number': 3300}, 'overall_precision': 0.5930386607250392, 'overall_recall': 0.6191374324154407, 'overall_f1': 0.6058070866141733, 'overall_accuracy': 0.8936436675475743} \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcd84db77c64b7e80b11faf83d5c948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [26:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------Result on test_set including all previous test sets------------------------------- \n",
      "\n",
      " {'ALL': {'precision': 0.6349206349206349, 'recall': 0.7100591715976331, 'f1': 0.6703910614525138, 'number': 169}, 'CAN': {'precision': 0.6359516616314199, 'recall': 0.6373959121877366, 'f1': 0.6366729678638943, 'number': 1321}, 'CHR': {'precision': 0.6936236391912908, 'recall': 0.7165809768637532, 'f1': 0.704915441757547, 'number': 3112}, 'TRE': {'precision': 0.6676008968609866, 'recall': 0.6878429107710078, 'f1': 0.6775707580713981, 'number': 3463}, 'overall_precision': 0.6718900675024108, 'overall_recall': 0.6911345319280843, 'overall_f1': 0.6813764439826416, 'overall_accuracy': 0.9272658074205129} \n",
      "\n",
      "\n",
      "------------------------------------Training and Evaluation complete--------------------------------------------------\n",
      "------------------------------------Model -> raunak6898/bert-finetuned-ner-all_data pushed to huggingface-hub ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_self_ls = []\n",
    "results_self_prev_ls = []\n",
    "\n",
    "results_self_ls_final = []\n",
    "results_self_prev_ls_final = []\n",
    "\n",
    "for iter in tqdm(range(len(model_ckpt_in))):\n",
    "    iter = 3        # comment for full pipeline run in one go\n",
    "    print(\"\\n---------------------------------------Task {} with base model -> {}-----------------------------------------\\n\".format(iter+1, model_ckpt_in[iter]))\n",
    "    model_ckpt = model_ckpt_in[iter]\n",
    "    model_ckpt_out = model_ckpt_eval[iter]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_ckpt)\n",
    "    data_collater = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_ckpt, id2label=id2label, label2id=label2id)\n",
    "    metric = evaluate.load('seqeval')\n",
    "\n",
    "    tokenized_datasets = ls_datadicts[0].map(\n",
    "        tokenize_and_align,\n",
    "        batched=True,\n",
    "        remove_columns=ls_datadicts[0][\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=model_ckpt_eval[iter],\n",
    "        evaluation_strategy='epoch',\n",
    "        # logging_steps=100,\n",
    "        # logging_strategy='steps',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=3,\n",
    "        use_mps_device=True,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=data_collater,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    mertic = evaluate.load('seqeval')\n",
    "    test_preds = trainer.predict(tokenized_datasets['test'])\n",
    "\n",
    "    for i in range(len(tokenized_datasets['test'])):\n",
    "        y_true = [id2label[id] for id in tokenized_datasets['test'][i]['labels'] if id != -100]\n",
    "        y_pred = [id2label[id] for id in np.argmax(test_preds[0][i], axis=1)[1:len(y_true)+1]]\n",
    "        metric.add_batch(predictions=[y_pred], references=[y_true])\n",
    "\n",
    "    results_self = metric.compute()                 \n",
    "    # results_self_ls.append(results_self)          # uncomment for full pipeline run in one go\n",
    "    results_self_ls_final.append(results_self)      # comment for full pipeline run in one go\n",
    "    print(\"\\n\\n-------------------------------------------------------------Result on test_set of this dataset ------------------------------------------------------------- \\n\\n\",(results_self),\"\\n\\n\")\n",
    "\n",
    "    metric = evaluate.load('seqeval')\n",
    "    test_preds_prev = trainer.predict(tokenized_datasets['test_prevs'])\n",
    "    for i in range(len(tokenized_datasets['test_prevs'])):\n",
    "        y_true = [id2label[id] for id in tokenized_datasets['test_prevs'][i]['labels'] if id != -100]\n",
    "        y_pred = [id2label[id] for id in np.argmax(test_preds_prev[0][i], axis=1)[1:len(y_true)+1]]\n",
    "        metric.add_batch(predictions=[y_pred], references=[y_true])\n",
    "\n",
    "    results_self_prevs = metric.compute()\n",
    "    # results_self_ls_prevs.append(results_self_prevs)          # uncomment for full pipeline run in one go\n",
    "    results_self_prev_ls_final.append(results_self_prevs)       # comment for full pipeline run in one go\n",
    "    \n",
    "    print(\"------------------------------------Result on test_set including all previous test sets------------------------------- \\n\\n\",results_self_prevs, \"\\n\\n\")\n",
    "\n",
    "    print(\"------------------------------------Training and Evaluation complete--------------------------------------------------\")\n",
    "    print(\"------------------------------------Model -> {} pushed to huggingface-hub ------------------------------------------\".format(model_ckpt_eval[iter]))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_test_f1s: \n",
      "\n",
      "                  T1+T2+T3\n",
      "allergy_name     0.585915\n",
      "cancer           0.565449\n",
      "chronic_disease  0.615013\n",
      "treatment        0.612305\n",
      "overall          0.605807 \n",
      "\n",
      "self_plus_prevs_test_f1s: \n",
      "\n",
      "                  T1+T2+T3\n",
      "allergy_name     0.670391\n",
      "cancer           0.636673\n",
      "chronic_disease  0.704915\n",
      "treatment        0.677571\n",
      "overall          0.681376\n"
     ]
    }
   ],
   "source": [
    "## takes output list of metric dictionaries and converts in desired table3 output as stated in problem statement.\n",
    "\n",
    "def measures_out(results_self_ls):\n",
    "    df = pd.DataFrame()\n",
    "    for i, dict in enumerate(results_self_ls):\n",
    "        # print(dict)\n",
    "        temp = {k.split('_')[-1]: v for k, v in dict.items() if k[0]=='o'}\n",
    "        temp['number'] = temp.pop('accuracy')\n",
    "        temp['number'] = 0\n",
    "        # print(temp)\n",
    "        one = {k :v for k, v in dict.items() if k[0]!='o'}\n",
    "        one['overall'] = temp\n",
    "        # print(one)\n",
    "        df = pd.concat([df, pd.DataFrame(one)])\n",
    "\n",
    "    index = [x for x in df.index.tolist() if x=='f1']\n",
    "    df = df.loc[index].drop_duplicates()\n",
    "    ['chronic_disease', 'cancer', 'treatment', 'allergy_name']\n",
    "    columns = {'ALL': 'allergy_name',\n",
    "            'CAN': 'cancer',\n",
    "            'TRE': 'treatment',\n",
    "            'CHR': 'chronic_disease'}\n",
    "    df = df.rename(columns=columns)\n",
    "    df.index = ['T1+T2+T3']\n",
    "    return df.transpose()\n",
    "\n",
    "print('self_test_f1s: \\n\\n', measures_out(results_self_ls_final),\"\\n\")\n",
    "\n",
    "print('self_plus_prevs_test_f1s: \\n\\n',measures_out(results_self_prev_ls_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measures_out(results_self_ls_final).to_excel('self_result.csv')\n",
    "# measures_out(results_self_prev_ls_final).to_excel('self_prevs_result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10191964285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ls = []\n",
    "for i in range(1,9):\n",
    "    ls.append(0.3/i)\n",
    "\n",
    "np.mean(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
